{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from td3_agent import AgentTD3\n",
    "from ddpg_agent import AgentDDPG\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if cuda enabled gpu is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select of the Unit Environment for the Continuous Control project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher_Linux_One/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_20/Reacher.x86_64')\n",
    "#env = UnityEnvironment(file_name='Crawler_Linux/Crawler.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(\n",
    "    states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_train(env, agent, cfg, brain_name, num_agents,\n",
    "                       print_every=1):\n",
    "    \"\"\"Actor Critic based Training\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        env                   : Environment\n",
    "        agent                 : Instance of Agent\n",
    "        cfg                   : Configuration of training \n",
    "        brain_name            : Name of brain\n",
    "        num_agents            : Number of Agents\n",
    "        print_every (int)     : interval to display results\n",
    "\n",
    "    \"\"\"\n",
    "    n_episodes = int(cfg['N_EPISODES'])\n",
    "    max_t = int(cfg['MAX_T'])\n",
    "    save_n_episodes = int(cfg['SAVE_N_EPISODES'])\n",
    "\n",
    "    # list of mean scores from each episode\n",
    "    mean_scores = []\n",
    "    # list of lowest scores from each episode\n",
    "    min_scores = []\n",
    "    # list of highest scores from each episode\n",
    "    max_scores = []\n",
    "    # mean scores from most recent episodes\n",
    "    df = pd.DataFrame(columns=['episode', 'duration',\n",
    "                               'min', 'max', 'std', 'mean'])\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset environment\n",
    "        # get current state for each agent\n",
    "        states = env_info.vector_observations\n",
    "        # initialize score for each agent\n",
    "        scores = np.zeros(num_agents)\n",
    "        agent.reset()\n",
    "        start_time = time.time()\n",
    "        for t in range(max_t):\n",
    "\n",
    "            # select an action\n",
    "            actions = agent.act(states, add_noise=True)\n",
    "            # send actions to environment\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations    # get next state\n",
    "            rewards = env_info.rewards                    # get reward\n",
    "            # see if episode has finished\n",
    "            dones = env_info.local_done\n",
    "\n",
    "            # save experience to replay buffer, perform learning step at\n",
    "            # defined interval\n",
    "            for state, action, reward, next_state, done in zip(states, actions,\n",
    "                                                               rewards,\n",
    "                                                               next_states,\n",
    "                                                               dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "\n",
    "            states = next_states\n",
    "\n",
    "            scores += rewards\n",
    "            if np.any(dones):  # exit loop when episode ends\n",
    "                break\n",
    "        # save time needed for episode\n",
    "        duration = time.time() - start_time\n",
    "        # save lowest score for a single agent\n",
    "        min_scores.append(np.min(scores))\n",
    "        # save highest score for a single agent\n",
    "        max_scores.append(np.max(scores))\n",
    "        # save mean score for the episode\n",
    "        mean_scores.append(np.mean(scores))\n",
    "\n",
    "        df.loc[i_episode-1] = [i_episode] + list([round(duration),\n",
    "                                                  np.min(scores),\n",
    "                                                  np.max(scores),\n",
    "                                                  np.std(scores),\n",
    "                                                  np.mean(scores)])\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {} ({} sec)  -- \\tMin: {:.1f}\\tMax: {:.1f}\\tMean: {:.1f}'.format(\n",
    "                  i_episode, round(duration), min_scores[-1], max_scores[-1],\n",
    "                  mean_scores[-1]))\n",
    "\n",
    "        if i_episode % save_n_episodes == 0:\n",
    "            epi_str = \"{:03}\".format(i_episode)\n",
    "            torch.save(agent.actor_local.state_dict(),\n",
    "                       agent.name+\"_\"+epi_str+\"_actor_ckpt.pth\")\n",
    "            torch.save(agent.critic_local.state_dict(),\n",
    "                       agent.name+\"_\"+epi_str+\"_critic_ckpt.pth\")\n",
    "    \n",
    "    torch.save(agent.actor_local.state_dict(),\n",
    "               agent.name+\"_final_actor_ckpt.pth\")\n",
    "    torch.save(agent.critic_local.state_dict(),\n",
    "               agent.name+\"_final_critic_ckpt.pth\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_test(env, agent, cfg, ckpt, n_episodes=100):\n",
    "    \"\"\"Actor Critic based Testing\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        env                   : Environment\n",
    "        agent                 : Instance of Agent\n",
    "        cfg                   : Configuration of training \n",
    "        ckpt                  : Number of checkpoint used for testing\n",
    "        n_episodes (int)      : Number of episodes for testing\n",
    "\n",
    "    \"\"\"    \n",
    "    df = pd.DataFrame(columns=['episode', 'duration',\n",
    "                               'min', 'max', 'std', 'mean'])\n",
    "\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(\n",
    "        states.shape[0], state_size))\n",
    "    print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "    ckpt_path = cfg['CKPT_PATH']\n",
    "    actor_path = '{}/{}_{}_actor_ckpt.pth'.format(ckpt_path, agent.name, ckpt)\n",
    "    critic_path = '{}/{}_{}_critic_ckpt.pth'.format(ckpt_path, agent.name, ckpt)\n",
    "    if not os.path.exists(actor_path) or not os.path.exists(critic_path):\n",
    "        print(\"Checkpoint(s) not found: {},{}\".format(actor_path, critic_path))\n",
    "        return df\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        agent.actor_local.load_state_dict(torch.load(actor_path))\n",
    "        agent.critic_local.load_state_dict(\n",
    "            torch.load(critic_path))\n",
    "    else:\n",
    "        agent.actor_local.load_state_dict(torch.load(\n",
    "            actor_path, map_location=lambda storage, loc: storage))\n",
    "        agent.critic_local.load_state_dict(torch.load(\n",
    "            critic_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    for i_episode in range(0, n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[\n",
    "            brain_name]     # reset the environment\n",
    "        # get the current state (for each agent)\n",
    "        states = env_info.vector_observations\n",
    "        # initialize the score (for each agent)\n",
    "        scores = np.zeros(num_agents)\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            # select an action\n",
    "            actions = agent.act(states, add_noise=False)\n",
    "            # send all actions to tne environment\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            # get next state (for each agent)\n",
    "            next_states = env_info.vector_observations\n",
    "            # get reward (for each agent)\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done  # see if episode finished\n",
    "            # update the score (for each agent)\n",
    "            scores += rewards\n",
    "            # roll over states to next time step\n",
    "            states = next_states\n",
    "            if np.any(dones):  # exit loop if episode finished\n",
    "                break\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        df.loc[i_episode] = [i_episode+1] + list([round(duration),\n",
    "                                                  np.min(scores),\n",
    "                                                  np.max(scores),\n",
    "                                                  np.std(scores),\n",
    "                                                  np.mean(scores)])\n",
    "\n",
    "        print('\\rEpisode {} ({} sec)\\tMean: {:.1f}'.format(\n",
    "            i_episode, round(duration), np.mean(scores)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(df):\n",
    "    \"\"\"Print min max plot of DQN Agent analytics\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        df :    Dataframe with scores\n",
    "    \"\"\"\n",
    "    ax = df.plot(x='episode', y='mean')\n",
    "    plt.fill_between(x='episode', y1='min', y2='max',\n",
    "                     color='lightgrey', data=df)\n",
    "    x_coordinates = [0, 150]\n",
    "    y_coordinates = [30, 30]\n",
    "    plt.plot(x_coordinates, y_coordinates, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = AgentDDPG(state_size=state_size, action_size=action_size,\n",
    "#                  random_seed=1, cfg_path=\"config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentTD3(state_size=state_size, action_size=action_size,\n",
    "                  random_seed=1, cfg_path=\"config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 (68 sec)  -- \tMin: 0.1\tMax: 2.1\tMean: 1.0\n",
      "Episode 2 (70 sec)  -- \tMin: 0.2\tMax: 3.4\tMean: 1.4\n"
     ]
    }
   ],
   "source": [
    "df = actor_critic_train(env, agent, agent.cfg, brain_name, num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = agent.cfg_items\n",
    "filename = 'data_{}.hdf5'.format(agent.name)\n",
    "store = pd.HDFStore(filename)\n",
    "store.put('dataset_01', df)\n",
    "store.get_storer('dataset_01').attrs.metadata = metadata\n",
    "store.close()\n",
    "plot_train(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
